# -*- coding: utf-8 -*-
"""task 3 lightning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107iw9JYRRJQX9qbv_gIpaE1p73fMBS4z
"""

import os
import random
import time
import numpy as np

import torch
import torch.nn as nn
import torch.utils.data
import torch.nn.functional as F

import torch.optim
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models

from resnet_wider import resnet50x1, resnet50x2, resnet50x4
import pytorch_lightning as pl
from pytorch_lightning import Trainer

# Hyperparameters
ARCH = 'resnet50-4x'
NUM_WORKERS = 2
NUM_GPUS = 1

BATCH_SIZE = 256
LEARNING_RATE = 0.1
WEIGHT_DECAY = 1e-6

DATASET_DIR = '../Datasets/CIFAR10'
IMAGE_SIZE = 224

train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])
test_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Logistic Regression defined in PyTorch
class LogisticRegression(nn.Module):
    def __init__(self, n_features, n_classes):
        super(LogisticRegression, self).__init__()
        self.model = nn.Linear(n_features, n_classes)

    def forward(self, x):
        return self.model(x)

class LitResNet(pl.LightningModule):

    def __init__(self):
        super(LitResNet, self).__init__()

        # Place your model initilization code here.
        self.num_classes = 10

        if ARCH == 'resnet50-1x':
            self.encoder = resnet50x1()
            sd = 'resnet50-1x.pth'
        elif ARCH == 'resnet50-2x':
            self.encoder = resnet50x2()
            sd = 'resnet50-2x.pth'
        elif ARCH == 'resnet50-4x':
            self.encoder = resnet50x4()
            sd = 'resnet50-4x.pth'
        else:
            raise NotImplementedError

        sd = torch.load(sd, map_location='cpu')
        self.encoder.load_state_dict(sd['state_dict'])

        for param in self.encoder.parameters():
            param.requires_grad = False

        num_ftrs = self.encoder.fc.in_features
        self.encoder.fc = nn.Identity()
        self.model = LogisticRegression(num_ftrs, self.num_classes)

    def forward(self, x):
        # # in lightning, forward defines the prediction/inference actions
        # images = batch['image']
        # labels = batch['label'].float()

        # outputs = self.model(images)
        # output_pred_proba = torch.sigmoid(outputs)

        # # return self.model(x)
        # return output_pred_proba, labels
        out = self.encoder(x)
        out = self.model(out)
        return out

    def loss_func(self):
        criterion = nn.CrossEntropyLoss()
        return criterion

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=LEARNING_RATE, 
            weight_decay=WEIGHT_DECAY
        )
        return optimizer

    def train_dataloader(self):
        # train_dataset = ImageData(image_list = train_img_idx, metadata_path = metadatapath, root_path = imagepath, train_mode = "train")
        # train_dl = DataLoader(train_dataset, batch_size = BATCH_SIZE, pin_memory = True, shuffle = True, num_workers = 2)
        # # train_dl = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True)

        # return train_dl
        train_dataset = datasets.CIFAR10(
            DATASET_DIR,
            train=True,
            download=True,
            transform=train_transform,
        )
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=BATCH_SIZE,
            shuffle=True,
            drop_last=True,
            num_workers=NUM_WORKERS,
        )
        return train_loader

    def test_dataloader(self):
        test_dataset = datasets.CIFAR10(
            DATASET_DIR,
            train=False,
            download=True,
            transform=test_transform,
        )
        test_loader = torch.utils.data.DataLoader(
            test_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            drop_last=True,
            num_workers=NUM_WORKERS,
        )
        return test_loader

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # It is independent of forward
        images, labels = batch
        #labels = labels.float()

        # Forward pass
        encodings = self.encoder(images)
        outputs = self.model(encodings)
        criterion = self.loss_func()
        loss = criterion(outputs, labels)

        tensorboard_logs = {'train_loss': loss}
        return {'loss': loss, 
                'log': tensorboard_logs}

    def test_step(self, batch, batch_idx):
        # images = batch['image']
        # labels = batch['label'].float()
        x, y = batch
        #labels = labels.float()
        x = x.view(x.size(0), -1)

        encodings = self.encoder(x)
        outputs = self.model(encodings)
        criterion = self.loss_func()
        loss = criterion(outputs, labels)

        y_hat = torch.argmax(outputs, dim=1)
        acc = torch.sum(y == y_hat).item() / (len(y) * 1.0)
        # y_hat = outputs.argmax(1)
        # acc = (y_pred == labels).sum().item() / labels.size(0)
        # output_pred_proba = torch.sigmoid(outputs)
        # return {'pred_proba':output_pred_proba, 'labels':labels }

        return {'test_loss': loss, 
                'test_acc': torch.tensor(acc)}

    # def test_epoch_end(self, test_step_outputs):
    #     # output of validation_step is passed into validation_epoch_end.
    #     # val_step_outputs is a list of dicts returns by validation_step, one entry per batch
    #     # prediction_probabilities = torch.tensor([], device=self.device)
    #     prediction_labels = torch.tensor([], device=self.device)

    #     for batch_result in test_step_outputs:
    #         prediction_probabilities = torch.cat(
    #             [prediction_probabilities, batch_result['pred_proba']])
    #         prediction_labels = torch.cat(
    #             [prediction_labels, batch_result['labels']])

    #     self.prediction_probabilities = prediction_probabilities
    #     self.prediction_labels = prediction_labels


if __name__ == '__main__':
    if NUM_GPUS > 1:
        trainer = Trainer(gpus=NUM_GPUS, accelerator='ddp', max_epochs=200)
    elif NUM_GPUS == 1:
        trainer = Trainer(gpus=NUM_GPUS, log_every_n_steps=50, max_epochs=200)
    else:
        trainer = Trainer(log_every_n_steps=50, max_epochs=200)

    model = LitResNet()
    trainer.fit(model)
    trainer.test(model)